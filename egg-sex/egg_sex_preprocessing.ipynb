{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls ./drive/MyDrive/CS163"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuUYNrb168HJ",
        "outputId": "2e1e904f-103b-4f9e-d5f5-3e92c63a5d20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Egg_Sex\t\t\t       Export_dataset_October24\t\t\tRetina_Fundus\n",
            "Export_dataset_November24.zip  ODIR-5K_Training_cropped_normal_Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./drive/MyDrive/CS163/Export_dataset_November24.zip -d ./unzipped"
      ],
      "metadata": {
        "id": "luLve8Yo6J_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHrcV6JSppeS",
        "outputId": "0df5326b-f04c-4f2d-f639-9aafdb1b3c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of data points in the constructed data structure: 1553\n"
          ]
        }
      ],
      "source": [
        "# Processing the csv file\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from random import shuffle\n",
        "import math\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "random.seed(1839)\n",
        "\n",
        "dir_to_data = \"./unzipped/HH19/\"\n",
        "\n",
        "\n",
        "csv_name = \"GT_labels_eggs_HH19.csv\"\n",
        "csv_path = os.path.join('./', csv_name)\n",
        "data_points = list()\n",
        "with open(csv_path, \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # to skip the header line\n",
        "    for row_number, row in enumerate(reader):\n",
        "        data_point_egg = dict()\n",
        "\n",
        "        data_point_egg['label'] = int(row[1])\n",
        "\n",
        "        if row[6] == '.':\n",
        "            img_name = f'img_{row[0]}.png'\n",
        "        else:\n",
        "            img_name = f'lowQ_img_{row[0]}.png'\n",
        "\n",
        "        data_point_egg['path'] = os.path.join(dir_to_data, row[6], img_name)\n",
        "\n",
        "        additional_info = dict()\n",
        "        additional_info['id'] = int(row[2])\n",
        "        additional_info['grade'] = int(row[3])\n",
        "        additional_info['real'] = int(row[4])\n",
        "        additional_info['stage'] = int(row[5])\n",
        "        data_point_egg['additional_info'] = additional_info\n",
        "\n",
        "        data_points.append(data_point_egg)\n",
        "\n",
        "data_points = [data_point for data_point in data_points if os.path.exists(data_point['path'])]\n",
        "\n",
        "print(f'Total number of data points in the constructed data structure: {len(data_points)}')  # Check if number of existing images are correct"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_hh19 = !ls -l ./unzipped/HH19/*.png 2>/dev/null | grep -c '^-'\n",
        "count_hh19_lower_quality = !ls -l ./unzipped/HH19/Lower_quality/*.png 2>/dev/null | grep -c '^-'\n",
        "\n",
        "print(f'Total number of existing HH19 images in directory: {int(count_hh19[0]) + int(count_hh19_lower_quality[0])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYiuDaWdvAOn",
        "outputId": "78d9209c-3ba3-4511-a7d9-ad810ccd0531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of existing HH19 images in directory: 1553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(1839)\n",
        "\n",
        "data_points_male = [data_point for data_point in data_points if data_point['label'] == 0]\n",
        "data_points_female = [data_point for data_point in data_points if data_point['label'] == 1]\n",
        "\n",
        "print(len(data_points_male) / len(data_points))\n",
        "\n",
        "def get_split_for_data_points(input_data_points):\n",
        "    id_count_dictionary = dict()\n",
        "    for data_point in input_data_points:\n",
        "        id = data_point['additional_info']['id']\n",
        "        id_count_dictionary[id] = id_count_dictionary.get(id, 0) + 1\n",
        "\n",
        "\n",
        "    print(sum(id_count_dictionary.values()))\n",
        "\n",
        "    id_count_dictionary = list(id_count_dictionary.items())\n",
        "    print(id_count_dictionary[0:10])\n",
        "    id_count_dictionary.sort()\n",
        "    shuffle(id_count_dictionary)\n",
        "    print(id_count_dictionary[0:10])\n",
        "\n",
        "    dataset_size = sum([data[1] for data in id_count_dictionary])\n",
        "    validation_size, test_size = math.ceil(0.125 * dataset_size), math.ceil(0.125 * dataset_size)\n",
        "    train_size = dataset_size - validation_size - test_size\n",
        "\n",
        "    img_counter = 0\n",
        "    index = 0\n",
        "    while img_counter < test_size:\n",
        "        img_counter += id_count_dictionary[index][1]\n",
        "        index += 1\n",
        "    test_slice = slice(0, index)\n",
        "\n",
        "    img_counter = 0\n",
        "    index_old = index\n",
        "    while img_counter < validation_size:\n",
        "        img_counter += id_count_dictionary[index][1]\n",
        "        index += 1\n",
        "    validation_slice = slice(index_old, index)\n",
        "\n",
        "    img_counter = 0\n",
        "    index_old = index\n",
        "    while index < len(id_count_dictionary):\n",
        "        img_counter += id_count_dictionary[index][1]\n",
        "        index += 1\n",
        "    train_slice = slice(index_old, index)\n",
        "\n",
        "    train_ids = [data[0] for data in id_count_dictionary[train_slice]]\n",
        "    validation_ids = [data[0] for data in id_count_dictionary[validation_slice]]\n",
        "    test_ids = [data[0] for data in id_count_dictionary[test_slice]]\n",
        "\n",
        "    data_points_train = [data_point for data_point in data_points if data_point['additional_info']['id'] in train_ids]\n",
        "    data_points_validation = [data_point for data_point in data_points if data_point['additional_info']['id'] in validation_ids]\n",
        "    data_points_test = [data_point for data_point in data_points if data_point['additional_info']['id'] in test_ids]\n",
        "\n",
        "    return data_points_train, data_points_validation, data_points_test, train_ids, validation_ids, test_ids\n",
        "\n",
        "data_points_train_male, data_points_validation_male, data_points_test_male, train_ids_male, validation_ids_male, test_ids_male = get_split_for_data_points(data_points_male)\n",
        "\n",
        "dataset_size = len(data_points_train_male) + len(data_points_validation_male) + len(data_points_test_male)\n",
        "print(f'train_size: {len(data_points_train_male) / dataset_size}, test_size: {len(data_points_test_male) / dataset_size}, validation_size: {len(data_points_validation_male) / dataset_size}')\n",
        "\n",
        "data_points_train_female, data_points_validation_female, data_points_test_female, train_ids_female, validation_ids_female, test_ids_female = get_split_for_data_points(data_points_female)\n",
        "\n",
        "dataset_size = len(data_points_train_female) + len(data_points_validation_female) + len(data_points_test_female)\n",
        "print(f'train_size: {len(data_points_train_female) / dataset_size}, test_size: {len(data_points_test_female) / dataset_size}, validation_size: {len(data_points_validation_female) / dataset_size}')\n",
        "\n",
        "data_points_train = data_points_train_male + data_points_train_female\n",
        "data_points_validation = data_points_validation_male + data_points_validation_female\n",
        "data_points_test = data_points_test_male + data_points_test_female\n",
        "\n",
        "dataset_size = len(data_points_train) + len(data_points_validation) + len(data_points_test)\n",
        "print(f'train_size: {len(data_points_train) / dataset_size}, test_size: {len(data_points_test) / dataset_size}, validation_size: {len(data_points_validation) / dataset_size}')\n",
        "\n",
        "\n",
        "\n",
        "# Check if same data exists in different parts of the split\n",
        "for data_point in data_points:\n",
        "    if (data_point in data_points_train) and (data_point in data_points_validation):\n",
        "        print(\"ERROR!!!\")\n",
        "    if (data_point in data_points_train) and (data_point in data_points_test):\n",
        "        print(\"ERROR!!!\")\n",
        "    if (data_point in data_points_validation) and (data_point in data_points_test):\n",
        "        print(\"ERROR!!!\")\n",
        "# Check if there are duplicates in a set\n",
        "print(any(data_points_train.count(x) > 1 for x in data_points_train))\n",
        "print(any(data_points_validation.count(x) > 1 for x in data_points_validation))\n",
        "print(any(data_points_test.count(x) > 1 for x in data_points_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh3PLQG3DQtA",
        "outputId": "151845c8-0efa-435e-ab6d-9fa88c3fe970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5112685125563425\n",
            "794\n",
            "[(10, 1), (16, 1), (20, 1), (22, 1), (25, 1), (27, 1), (28, 1), (45, 1), (46, 1), (48, 1)]\n",
            "[(610, 1), (147, 1), (544, 1), (1284, 1), (709, 2), (1084, 1), (1114, 1), (998, 1), (1065, 1), (1264, 2)]\n",
            "train_size: 0.7468513853904282, test_size: 0.12720403022670027, validation_size: 0.12594458438287154\n",
            "759\n",
            "[(1, 1), (2, 1), (8, 1), (26, 1), (30, 1), (32, 1), (34, 1), (36, 1), (39, 1), (40, 1)]\n",
            "[(174, 1), (412, 1), (236, 1), (1280, 1), (192, 1), (752, 3), (72, 1), (1226, 3), (172, 1), (723, 2)]\n",
            "train_size: 0.7483530961791831, test_size: 0.12648221343873517, validation_size: 0.1251646903820817\n",
            "train_size: 0.7475853187379266, test_size: 0.12685125563425628, validation_size: 0.12556342562781714\n",
            "False\n",
            "False\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(1839)\n",
        "\n",
        "# Processing the images\n",
        "\n",
        "img_size = 512\n",
        "pickle_chunk_size = 5000\n",
        "\n",
        "def load_and_process_image(file_path, img_size):\n",
        "    try:\n",
        "        # image = Image.open(file_path)\n",
        "        # image.save('./image_sample.png')\n",
        "\n",
        "        image = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
        "        # cv2.imwrite('./image_sample.png', image)\n",
        "\n",
        "        image = (image / 256).astype(np.uint8)  # 16-bit to 8-bit\n",
        "        image = Image.fromarray(image)\n",
        "        # image.save('./image_sample.png')\n",
        "\n",
        "        shorter_edge = min(image.size)\n",
        "\n",
        "        # transforms.ToTensor() is removed for the pickle size to be smaller\n",
        "        preprocess = transforms.Compose([\n",
        "            transforms.CenterCrop(shorter_edge),\n",
        "            transforms.Resize((img_size, img_size))\n",
        "        ])\n",
        "\n",
        "        image = preprocess(image)\n",
        "        return image\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error loading the image: {e}')\n",
        "        return None\n",
        "\n",
        "\n",
        "def chunks(input_list, chunk_size):\n",
        "    for i in range(0, len(input_list), chunk_size):\n",
        "        yield input_list[i:i+chunk_size]\n",
        "\n",
        "\n",
        "mode = 'validation'\n",
        "data_points = data_points_validation\n",
        "\n",
        "data_points_chunks_generator = chunks(data_points, pickle_chunk_size)\n",
        "\n",
        "output_dir = os.path.join(f'./processed_data/HH19/{mode}')\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for chunk_no, chunk in enumerate(data_points_chunks_generator):\n",
        "    dataset_part = list()\n",
        "    for chunk_entry_no, chunk_entry in enumerate(chunk):\n",
        "        if (chunk_entry_no + 1) % 10 == 0:\n",
        "            print(f'Processing chunk entry no: {chunk_entry_no + 1}')\n",
        "\n",
        "        image = load_and_process_image(chunk_entry['path'], img_size)\n",
        "        data_point = {'label': chunk_entry['label'], 'image': image, 'additional_info': chunk_entry['additional_info']}\n",
        "        dataset_part.append(data_point)\n",
        "\n",
        "    with open(os.path.join(output_dir, f'{mode}_{img_size}_part_{chunk_no + 1}.pickle'), 'wb') as f:\n",
        "        pickle.dump(dataset_part, f)\n",
        "\n",
        "mode = 'testing'\n",
        "data_points = data_points_test\n",
        "\n",
        "data_points_chunks_generator = chunks(data_points, pickle_chunk_size)\n",
        "\n",
        "output_dir = os.path.join(f'./processed_data/HH19/{mode}')\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for chunk_no, chunk in enumerate(data_points_chunks_generator):\n",
        "    dataset_part = list()\n",
        "    for chunk_entry_no, chunk_entry in enumerate(chunk):\n",
        "        if (chunk_entry_no + 1) % 10 == 0:\n",
        "            print(f'Processing chunk entry no: {chunk_entry_no + 1}')\n",
        "\n",
        "        image = load_and_process_image(chunk_entry['path'], img_size)\n",
        "        data_point = {'label': chunk_entry['label'], 'image': image, 'additional_info': chunk_entry['additional_info']}\n",
        "        dataset_part.append(data_point)\n",
        "\n",
        "    with open(os.path.join(output_dir, f'{mode}_{img_size}_part_{chunk_no + 1}.pickle'), 'wb') as f:\n",
        "        pickle.dump(dataset_part, f)\n",
        "\n",
        "mode = 'training'\n",
        "data_points = data_points_train\n",
        "\n",
        "data_points_chunks_generator = chunks(data_points, pickle_chunk_size)\n",
        "\n",
        "output_dir = os.path.join(f'./processed_data/HH19/{mode}')\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for chunk_no, chunk in enumerate(data_points_chunks_generator):\n",
        "    dataset_part = list()\n",
        "    for chunk_entry_no, chunk_entry in enumerate(chunk):\n",
        "        if (chunk_entry_no + 1) % 10 == 0:\n",
        "            print(f'Processing chunk entry no: {chunk_entry_no + 1}')\n",
        "\n",
        "        image = load_and_process_image(chunk_entry['path'], img_size)\n",
        "        data_point = {'label': chunk_entry['label'], 'image': image, 'additional_info': chunk_entry['additional_info']}\n",
        "        dataset_part.append(data_point)\n",
        "\n",
        "    with open(os.path.join(output_dir, f'{mode}_{img_size}_part_{chunk_no + 1}.pickle'), 'wb') as f:\n",
        "        pickle.dump(dataset_part, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifKCq2w1Nn8A",
        "outputId": "f09a395a-b5d7-44cd-e551-fa2812963018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk entry no: 10\n",
            "Processing chunk entry no: 20\n",
            "Processing chunk entry no: 30\n",
            "Processing chunk entry no: 40\n",
            "Processing chunk entry no: 50\n",
            "Processing chunk entry no: 60\n",
            "Processing chunk entry no: 70\n",
            "Processing chunk entry no: 80\n",
            "Processing chunk entry no: 90\n",
            "Processing chunk entry no: 100\n",
            "Processing chunk entry no: 110\n",
            "Processing chunk entry no: 120\n",
            "Processing chunk entry no: 130\n",
            "Processing chunk entry no: 140\n",
            "Processing chunk entry no: 150\n",
            "Processing chunk entry no: 160\n",
            "Processing chunk entry no: 170\n",
            "Processing chunk entry no: 180\n",
            "Processing chunk entry no: 190\n",
            "Processing chunk entry no: 10\n",
            "Processing chunk entry no: 20\n",
            "Processing chunk entry no: 30\n",
            "Processing chunk entry no: 40\n",
            "Processing chunk entry no: 50\n",
            "Processing chunk entry no: 60\n",
            "Processing chunk entry no: 70\n",
            "Processing chunk entry no: 80\n",
            "Processing chunk entry no: 90\n",
            "Processing chunk entry no: 100\n",
            "Processing chunk entry no: 110\n",
            "Processing chunk entry no: 120\n",
            "Processing chunk entry no: 130\n",
            "Processing chunk entry no: 140\n",
            "Processing chunk entry no: 150\n",
            "Processing chunk entry no: 160\n",
            "Processing chunk entry no: 170\n",
            "Processing chunk entry no: 180\n",
            "Processing chunk entry no: 190\n",
            "Processing chunk entry no: 10\n",
            "Processing chunk entry no: 20\n",
            "Processing chunk entry no: 30\n",
            "Processing chunk entry no: 40\n",
            "Processing chunk entry no: 50\n",
            "Processing chunk entry no: 60\n",
            "Processing chunk entry no: 70\n",
            "Processing chunk entry no: 80\n",
            "Processing chunk entry no: 90\n",
            "Processing chunk entry no: 100\n",
            "Processing chunk entry no: 110\n",
            "Processing chunk entry no: 120\n",
            "Processing chunk entry no: 130\n",
            "Processing chunk entry no: 140\n",
            "Processing chunk entry no: 150\n",
            "Processing chunk entry no: 160\n",
            "Processing chunk entry no: 170\n",
            "Processing chunk entry no: 180\n",
            "Processing chunk entry no: 190\n",
            "Processing chunk entry no: 200\n",
            "Processing chunk entry no: 210\n",
            "Processing chunk entry no: 220\n",
            "Processing chunk entry no: 230\n",
            "Processing chunk entry no: 240\n",
            "Processing chunk entry no: 250\n",
            "Processing chunk entry no: 260\n",
            "Processing chunk entry no: 270\n",
            "Processing chunk entry no: 280\n",
            "Processing chunk entry no: 290\n",
            "Processing chunk entry no: 300\n",
            "Processing chunk entry no: 310\n",
            "Processing chunk entry no: 320\n",
            "Processing chunk entry no: 330\n",
            "Processing chunk entry no: 340\n",
            "Processing chunk entry no: 350\n",
            "Processing chunk entry no: 360\n",
            "Processing chunk entry no: 370\n",
            "Processing chunk entry no: 380\n",
            "Processing chunk entry no: 390\n",
            "Processing chunk entry no: 400\n",
            "Processing chunk entry no: 410\n",
            "Processing chunk entry no: 420\n",
            "Processing chunk entry no: 430\n",
            "Processing chunk entry no: 440\n",
            "Processing chunk entry no: 450\n",
            "Processing chunk entry no: 460\n",
            "Processing chunk entry no: 470\n",
            "Processing chunk entry no: 480\n",
            "Processing chunk entry no: 490\n",
            "Processing chunk entry no: 500\n",
            "Processing chunk entry no: 510\n",
            "Processing chunk entry no: 520\n",
            "Processing chunk entry no: 530\n",
            "Processing chunk entry no: 540\n",
            "Processing chunk entry no: 550\n",
            "Processing chunk entry no: 560\n",
            "Processing chunk entry no: 570\n",
            "Processing chunk entry no: 580\n",
            "Processing chunk entry no: 590\n",
            "Processing chunk entry no: 600\n",
            "Processing chunk entry no: 610\n",
            "Processing chunk entry no: 620\n",
            "Processing chunk entry no: 630\n",
            "Processing chunk entry no: 640\n",
            "Processing chunk entry no: 650\n",
            "Processing chunk entry no: 660\n",
            "Processing chunk entry no: 670\n",
            "Processing chunk entry no: 680\n",
            "Processing chunk entry no: 690\n",
            "Processing chunk entry no: 700\n",
            "Processing chunk entry no: 710\n",
            "Processing chunk entry no: 720\n",
            "Processing chunk entry no: 730\n",
            "Processing chunk entry no: 740\n",
            "Processing chunk entry no: 750\n",
            "Processing chunk entry no: 760\n",
            "Processing chunk entry no: 770\n",
            "Processing chunk entry no: 780\n",
            "Processing chunk entry no: 790\n",
            "Processing chunk entry no: 800\n",
            "Processing chunk entry no: 810\n",
            "Processing chunk entry no: 820\n",
            "Processing chunk entry no: 830\n",
            "Processing chunk entry no: 840\n",
            "Processing chunk entry no: 850\n",
            "Processing chunk entry no: 860\n",
            "Processing chunk entry no: 870\n",
            "Processing chunk entry no: 880\n",
            "Processing chunk entry no: 890\n",
            "Processing chunk entry no: 900\n",
            "Processing chunk entry no: 910\n",
            "Processing chunk entry no: 920\n",
            "Processing chunk entry no: 930\n",
            "Processing chunk entry no: 940\n",
            "Processing chunk entry no: 950\n",
            "Processing chunk entry no: 960\n",
            "Processing chunk entry no: 970\n",
            "Processing chunk entry no: 980\n",
            "Processing chunk entry no: 990\n",
            "Processing chunk entry no: 1000\n",
            "Processing chunk entry no: 1010\n",
            "Processing chunk entry no: 1020\n",
            "Processing chunk entry no: 1030\n",
            "Processing chunk entry no: 1040\n",
            "Processing chunk entry no: 1050\n",
            "Processing chunk entry no: 1060\n",
            "Processing chunk entry no: 1070\n",
            "Processing chunk entry no: 1080\n",
            "Processing chunk entry no: 1090\n",
            "Processing chunk entry no: 1100\n",
            "Processing chunk entry no: 1110\n",
            "Processing chunk entry no: 1120\n",
            "Processing chunk entry no: 1130\n",
            "Processing chunk entry no: 1140\n",
            "Processing chunk entry no: 1150\n",
            "Processing chunk entry no: 1160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_ids_male)\n",
        "print(train_ids_male_hh19)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2WRH8GrGU-8",
        "outputId": "03123953-a0ec-4290-928f-7cd47b0313b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1221, 648, 386, 186, 976, 775, 903, 828, 486, 744, 603, 707, 854, 234, 38, 1003, 534, 724, 877, 1077, 930, 310, 635, 1224, 1013, 1292, 48, 28, 469, 684, 420, 1076, 218, 1234, 1010, 1169, 609, 330, 720, 353, 979, 1122, 1263, 1142, 22, 602, 113, 769, 166, 375, 292, 873, 785, 1196, 855, 868, 1233, 594, 421, 1303, 1216, 133, 860, 747, 1073, 729, 481, 1074, 751, 1051, 160, 97, 411, 794, 1183, 546, 359, 400, 409, 558, 730, 376, 929, 468, 963, 990, 574, 654, 318, 987, 913, 633, 874, 631, 588, 276, 235, 1230, 801, 846, 500, 1105, 1136, 492, 1002, 157, 245, 199, 535, 832, 307, 577, 935, 1036, 659, 4, 560, 1291, 61, 1127, 46, 939, 1212, 869, 262, 1050, 308, 975, 1048, 119, 1123, 693, 790, 932, 531, 1018, 933, 674, 740, 137, 20, 444, 134, 822, 168, 1000, 925, 149, 148, 798, 45, 95, 213, 580, 583, 1338, 1004, 102, 196, 655, 493, 71, 506, 756, 797, 1124, 1057, 619, 657, 111, 1159, 909, 1078, 576, 244, 830, 1186, 726, 263, 920, 922, 678, 459, 817, 89, 581, 373, 1009, 1069, 461, 16, 25, 84, 209, 460, 439, 1020, 185, 92, 487, 1277, 350, 1008, 714, 788, 665, 904, 445, 607, 575, 1360, 449, 349, 600, 1177, 132, 1231, 296, 872, 862, 116, 1229, 554, 1235, 910, 369, 146, 974, 337, 919, 761, 878, 662, 818, 993, 663, 443, 447, 688, 530, 537, 1126, 484, 512, 786, 1202, 1115, 1071, 733, 770, 465, 812, 453, 1064, 419, 653, 120, 380, 1019, 853, 533, 344, 596, 430, 838, 1044, 122, 692, 225, 325, 743, 1134, 802, 17, 1098, 827, 1032, 112, 1081, 1161, 978, 183, 509, 187, 1117, 1295, 247, 753, 1175, 1331, 309, 777, 1111, 182, 457, 1022, 623, 888, 1121, 890, 690, 651, 1274, 195, 472, 1306, 1260, 118, 625, 207, 1227, 273, 898, 128, 1290, 727, 549, 1028, 739, 883, 1070, 297, 553, 841, 622, 142, 1302, 758, 567, 699, 880, 391, 340, 88, 379, 1034, 840, 1055, 741, 1047, 1361, 539, 1358, 431, 417, 237, 490, 879, 283, 736, 671, 1332, 271, 323, 284, 996, 442, 863, 1015, 377, 1300, 1091, 11, 1301, 175, 927, 348, 504, 742, 763, 1334, 410, 652, 735, 571, 563, 884, 232, 372, 193, 807, 592, 396, 1012, 826, 937, 1025, 441, 254, 962, 759, 1083, 858, 123, 1016, 51, 859, 1209, 49, 908, 907, 618, 806, 1156, 86, 433, 627, 249, 327, 470, 1194, 845, 1199, 573, 326, 856, 217, 1298, 833, 1170, 550, 1093, 211, 1007, 1066, 1178, 675, 269, 1172, 1075, 1237, 1243, 7, 964, 448, 917, 605, 1242, 87, 81, 27, 629, 914, 278, 188, 190, 1250, 135]\n",
            "[1221, 648, 386, 186, 976, 775, 903, 828, 486, 744, 603, 707, 854, 234, 38, 1003, 534, 724, 877, 1077, 930, 310, 635, 1224, 1013, 1292, 48, 28, 469, 684, 420, 1076, 218, 1234, 1010, 1169, 609, 330, 720, 353, 979, 1122, 1263, 1142, 22, 602, 113, 769, 166, 375, 292, 873, 785, 1196, 855, 868, 1233, 594, 421, 1303, 1216, 133, 860, 747, 1073, 729, 481, 1074, 751, 1051, 160, 97, 411, 794, 1183, 546, 359, 400, 409, 558, 730, 376, 929, 468, 963, 990, 574, 654, 318, 987, 913, 633, 874, 631, 588, 276, 235, 1230, 801, 846, 500, 1105, 1136, 492, 1002, 157, 245, 199, 535, 832, 307, 577, 935, 1036, 659, 4, 560, 1291, 61, 1127, 46, 939, 1212, 869, 262, 1050, 308, 975, 1048, 119, 1123, 693, 790, 932, 531, 1018, 933, 674, 740, 137, 20, 444, 134, 822, 168, 1000, 925, 149, 148, 798, 45, 95, 213, 580, 583, 1338, 1004, 102, 196, 655, 493, 71, 506, 756, 797, 1124, 1057, 619, 657, 111, 1159, 909, 1078, 576, 244, 830, 1186, 726, 263, 920, 922, 678, 459, 817, 89, 581, 373, 1009, 1069, 461, 16, 25, 84, 209, 460, 439, 1020, 185, 92, 487, 1277, 350, 1008, 714, 788, 665, 904, 445, 607, 575, 1360, 449, 349, 600, 1177, 132, 1231, 296, 872, 862, 116, 1229, 554, 1235, 910, 369, 146, 974, 337, 919, 761, 878, 662, 818, 993, 663, 443, 447, 688, 530, 537, 1126, 484, 512, 786, 1202, 1115, 1071, 733, 770, 465, 812, 453, 1064, 419, 653, 120, 380, 1019, 853, 533, 344, 596, 430, 838, 1044, 122, 692, 225, 325, 743, 1134, 802, 17, 1098, 827, 1032, 112, 1081, 1161, 978, 183, 509, 187, 1117, 1295, 247, 753, 1175, 1331, 309, 777, 1111, 182, 457, 1022, 623, 888, 1121, 890, 690, 651, 1274, 195, 472, 1306, 1260, 118, 625, 207, 1227, 273, 898, 128, 1290, 727, 549, 1028, 739, 883, 1070, 297, 553, 841, 622, 142, 1302, 758, 567, 699, 880, 391, 340, 88, 379, 1034, 840, 1055, 741, 1047, 1361, 539, 1358, 431, 417, 237, 490, 879, 283, 736, 671, 1332, 271, 323, 284, 996, 442, 863, 1015, 377, 1300, 1091, 11, 1301, 175, 927, 348, 504, 742, 763, 1334, 410, 652, 735, 571, 563, 884, 232, 372, 193, 807, 592, 396, 1012, 826, 937, 1025, 441, 254, 962, 759, 1083, 858, 123, 1016, 51, 859, 1209, 49, 908, 907, 618, 806, 1156, 86, 433, 627, 249, 327, 470, 1194, 845, 1199, 573, 326, 856, 217, 1298, 833, 1170, 550, 1093, 211, 1007, 1066, 1178, 675, 269, 1172, 1075, 1237, 1243, 7, 964, 448, 917, 605, 1242, 87, 81, 27, 629, 914, 278, 188, 190, 1250, 135]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_ids_male_hh19 = train_ids_male\n",
        "print(len(train_ids_male_hh19))\n",
        "train_ids_female_hh19 = train_ids_female\n",
        "print(len(train_ids_female_hh19))\n",
        "\n",
        "validation_ids_male_hh19 = validation_ids_male\n",
        "print(len(validation_ids_male_hh19))\n",
        "validation_ids_female_hh19 = validation_ids_female\n",
        "print(len(validation_ids_female_hh19))\n",
        "\n",
        "test_ids_male_hh19 = test_ids_male\n",
        "print(len(test_ids_male_hh19))\n",
        "test_ids_female_hh19 = test_ids_female\n",
        "print(len(test_ids_female_hh19))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxQt9dA1D1fa",
        "outputId": "65ca9c12-9ffe-4702-d775-d7e508e878b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "448\n",
            "403\n",
            "73\n",
            "74\n",
            "77\n",
            "66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_to_data = \"./unzipped/HH25/\"\n",
        "\n",
        "csv_name = \"GT_labels_eggs_HH25.csv\"\n",
        "csv_path = os.path.join('./', csv_name)\n",
        "data_points = list()\n",
        "with open(csv_path, \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # to skip the header line\n",
        "    for row_number, row in enumerate(reader):\n",
        "        data_point_egg = dict()\n",
        "\n",
        "        data_point_egg['label'] = int(row[1])\n",
        "\n",
        "        if row[6] == '.':\n",
        "            img_name = f'img_{row[0]}.png'\n",
        "        else:\n",
        "            img_name = f'lowQ_img_{row[0]}.png'\n",
        "\n",
        "        data_point_egg['path'] = os.path.join(dir_to_data, row[6], img_name)\n",
        "\n",
        "        additional_info = dict()\n",
        "        additional_info['id'] = int(row[2])\n",
        "        additional_info['grade'] = int(row[3])\n",
        "        additional_info['real'] = int(row[4])\n",
        "        additional_info['stage'] = int(row[5])\n",
        "        data_point_egg['additional_info'] = additional_info\n",
        "\n",
        "        data_points.append(data_point_egg)\n",
        "\n",
        "data_points = [data_point for data_point in data_points if os.path.exists(data_point['path'])]\n",
        "\n",
        "print(f'Total number of data points in the constructed data structure: {len(data_points)}')  # Check if number of existing images are correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gIwKs7QEmK4",
        "outputId": "fdc8b2de-4984-4772-8aad-64a54270078b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of data points in the constructed data structure: 1220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_hh25 = !ls -l ./unzipped/HH25/*.png 2>/dev/null | grep -c '^-'\n",
        "count_hh25_lower_quality = !ls -l ./unzipped/HH25/Lower_quality/*.png 2>/dev/null | grep -c '^-'\n",
        "\n",
        "print(f'Total number of existing HH25 images in directory: {int(count_hh25[0]) + int(count_hh25_lower_quality[0])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27syqhglFHLm",
        "outputId": "ad3fe9ff-ebbe-4ef8-d505-edf93e96bb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of existing HH25 images in directory: 1220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(1839)\n",
        "\n",
        "data_points_male = [data_point for data_point in data_points if data_point['label'] == 0]\n",
        "data_points_female = [data_point for data_point in data_points if data_point['label'] == 1]\n",
        "\n",
        "print(len(data_points_male) / len(data_points))\n",
        "\n",
        "def get_split_for_data_points(input_data_points, train_hh19, validation_hh19, test_hh19):\n",
        "    id_count_dictionary = dict()\n",
        "    for data_point in input_data_points:\n",
        "        id = data_point['additional_info']['id']\n",
        "        id_count_dictionary[id] = id_count_dictionary.get(id, 0) + 1\n",
        "\n",
        "\n",
        "    print(sum(id_count_dictionary.values()))\n",
        "\n",
        "    id_count_dictionary = list(id_count_dictionary.items())\n",
        "    id_count_dictionary.sort()\n",
        "    shuffle(id_count_dictionary)\n",
        "\n",
        "    in_train_hh19 = [item for item in id_count_dictionary if item[0] in train_hh19]\n",
        "    id_count_dictionary = [item for item in id_count_dictionary if item not in in_train_hh19]\n",
        "\n",
        "    in_validation_hh19 = [item for item in id_count_dictionary if item[0] in validation_hh19]\n",
        "    id_count_dictionary = [item for item in id_count_dictionary if item not in in_validation_hh19]\n",
        "\n",
        "    in_test_hh19 = [item for item in id_count_dictionary if item[0] in test_hh19]\n",
        "    id_count_dictionary = [item for item in id_count_dictionary if item not in in_test_hh19]\n",
        "\n",
        "    dataset_size = sum([data[1] for data in id_count_dictionary])\n",
        "    validation_size, test_size = math.ceil(0.125 * dataset_size), math.ceil(0.125 * dataset_size)\n",
        "    train_size = dataset_size - validation_size - test_size\n",
        "\n",
        "    img_counter = 0\n",
        "    index = 0\n",
        "    while img_counter < test_size:\n",
        "        img_counter += id_count_dictionary[index][1]\n",
        "        index += 1\n",
        "    test_slice = slice(0, index)\n",
        "\n",
        "    img_counter = 0\n",
        "    index_old = index\n",
        "    while img_counter < validation_size:\n",
        "        img_counter += id_count_dictionary[index][1]\n",
        "        index += 1\n",
        "    validation_slice = slice(index_old, index)\n",
        "\n",
        "    img_counter = 0\n",
        "    index_old = index\n",
        "    while index < len(id_count_dictionary):\n",
        "        img_counter += id_count_dictionary[index][1]\n",
        "        index += 1\n",
        "    train_slice = slice(index_old, index)\n",
        "\n",
        "    train_ids = [data[0] for data in id_count_dictionary[train_slice]]\n",
        "    validation_ids = [data[0] for data in id_count_dictionary[validation_slice]]\n",
        "    test_ids = [data[0] for data in id_count_dictionary[test_slice]]\n",
        "\n",
        "    data_points_train = [data_point for data_point in data_points if data_point['additional_info']['id'] in (train_ids + train_hh19)]\n",
        "    data_points_validation = [data_point for data_point in data_points if data_point['additional_info']['id'] in (validation_ids + validation_hh19)]\n",
        "    data_points_test = [data_point for data_point in data_points if data_point['additional_info']['id'] in (test_ids + test_hh19)]\n",
        "\n",
        "    return data_points_train, data_points_validation, data_points_test, train_ids, validation_ids, test_ids\n",
        "\n",
        "data_points_train_male, data_points_validation_male, data_points_test_male, train_ids_male, validation_ids_male, test_ids_male = get_split_for_data_points(data_points_male, train_ids_male_hh19, validation_ids_male_hh19, test_ids_male_hh19)\n",
        "\n",
        "dataset_size = len(data_points_train_male) + len(data_points_validation_male) + len(data_points_test_male)\n",
        "print(f'train_size: {len(data_points_train_male) / dataset_size}, test_size: {len(data_points_test_male) / dataset_size}, validation_size: {len(data_points_validation_male) / dataset_size}')\n",
        "\n",
        "data_points_train_female, data_points_validation_female, data_points_test_female, train_ids_female, validation_ids_female, test_ids_female = get_split_for_data_points(data_points_female, train_ids_female_hh19, validation_ids_female_hh19, test_ids_female_hh19)\n",
        "\n",
        "dataset_size = len(data_points_train_female) + len(data_points_validation_female) + len(data_points_test_female)\n",
        "print(f'train_size: {len(data_points_train_female) / dataset_size}, test_size: {len(data_points_test_female) / dataset_size}, validation_size: {len(data_points_validation_female) / dataset_size}')\n",
        "\n",
        "data_points_train = data_points_train_male + data_points_train_female\n",
        "data_points_validation = data_points_validation_male + data_points_validation_female\n",
        "data_points_test = data_points_test_male + data_points_test_female\n",
        "\n",
        "dataset_size = len(data_points_train) + len(data_points_validation) + len(data_points_test)\n",
        "print(f'train_size: {len(data_points_train) / dataset_size}, test_size: {len(data_points_test) / dataset_size}, validation_size: {len(data_points_validation) / dataset_size}')\n",
        "\n",
        "print(len(data_points_train), len(data_points_validation), len(data_points_test))\n",
        "\n",
        "\n",
        "# Check if same data exists in different parts of the split\n",
        "for data_point in data_points:\n",
        "    if (data_point in data_points_train) and (data_point in data_points_validation):\n",
        "        print(\"ERROR!!!\")\n",
        "    if (data_point in data_points_train) and (data_point in data_points_test):\n",
        "        print(\"ERROR!!!\")\n",
        "    if (data_point in data_points_validation) and (data_point in data_points_test):\n",
        "        print(\"ERROR!!!\")\n",
        "# Check if there are duplicates in a set\n",
        "print(any(data_points_train.count(x) > 1 for x in data_points_train))\n",
        "print(any(data_points_validation.count(x) > 1 for x in data_points_validation))\n",
        "print(any(data_points_test.count(x) > 1 for x in data_points_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt8oRotyFPi_",
        "outputId": "acfb87ad-91ab-4ee2-9869-9a1de2f2988e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5172131147540984\n",
            "631\n",
            "train_size: 0.7179080824088748, test_size: 0.13946117274167988, validation_size: 0.14263074484944532\n",
            "589\n",
            "train_size: 0.7402376910016978, test_size: 0.12563667232597622, validation_size: 0.13412563667232597\n",
            "train_size: 0.728688524590164, test_size: 0.13278688524590163, validation_size: 0.13852459016393442\n",
            "889 169 162\n",
            "False\n",
            "False\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_points_train), len(data_points_validation), len(data_points_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUl5P9C7RmBA",
        "outputId": "94e693e0-f108-438c-b283-943c17ed663c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "889 169 162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(1839)\n",
        "\n",
        "# Processing the images\n",
        "\n",
        "img_size = 512\n",
        "pickle_chunk_size = 5000\n",
        "\n",
        "def load_and_process_image(file_path, img_size):\n",
        "    try:\n",
        "        # image = Image.open(file_path)\n",
        "        # image.save('./image_sample.png')\n",
        "\n",
        "        image = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
        "        # cv2.imwrite('./image_sample.png', image)\n",
        "\n",
        "        image = (image / 256).astype(np.uint8)  # 16-bit to 8-bit\n",
        "        image = Image.fromarray(image)\n",
        "        # image.save('./image_sample.png')\n",
        "\n",
        "        shorter_edge = min(image.size)\n",
        "\n",
        "        # transforms.ToTensor() is removed for the pickle size to be smaller\n",
        "        preprocess = transforms.Compose([\n",
        "            transforms.CenterCrop(shorter_edge),\n",
        "            transforms.Resize((img_size, img_size))\n",
        "        ])\n",
        "\n",
        "        image = preprocess(image)\n",
        "        return image\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error loading the image: {e}')\n",
        "        return None\n",
        "\n",
        "\n",
        "def chunks(input_list, chunk_size):\n",
        "    for i in range(0, len(input_list), chunk_size):\n",
        "        yield input_list[i:i+chunk_size]\n",
        "\n",
        "\n",
        "mode = 'validation'\n",
        "data_points = data_points_validation\n",
        "\n",
        "data_points_chunks_generator = chunks(data_points, pickle_chunk_size)\n",
        "\n",
        "output_dir = os.path.join(f'./processed_data/HH25/{mode}')\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for chunk_no, chunk in enumerate(data_points_chunks_generator):\n",
        "    dataset_part = list()\n",
        "    for chunk_entry_no, chunk_entry in enumerate(chunk):\n",
        "        if (chunk_entry_no + 1) % 10 == 0:\n",
        "            print(f'Processing chunk entry no: {chunk_entry_no + 1}')\n",
        "\n",
        "        image = load_and_process_image(chunk_entry['path'], img_size)\n",
        "        data_point = {'label': chunk_entry['label'], 'image': image, 'additional_info': chunk_entry['additional_info']}\n",
        "        dataset_part.append(data_point)\n",
        "\n",
        "    with open(os.path.join(output_dir, f'{mode}_{img_size}_part_{chunk_no + 1}.pickle'), 'wb') as f:\n",
        "        pickle.dump(dataset_part, f)\n",
        "\n",
        "mode = 'testing'\n",
        "data_points = data_points_test\n",
        "\n",
        "data_points_chunks_generator = chunks(data_points, pickle_chunk_size)\n",
        "\n",
        "output_dir = os.path.join(f'./processed_data/HH25/{mode}')\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for chunk_no, chunk in enumerate(data_points_chunks_generator):\n",
        "    dataset_part = list()\n",
        "    for chunk_entry_no, chunk_entry in enumerate(chunk):\n",
        "        if (chunk_entry_no + 1) % 10 == 0:\n",
        "            print(f'Processing chunk entry no: {chunk_entry_no + 1}')\n",
        "\n",
        "        image = load_and_process_image(chunk_entry['path'], img_size)\n",
        "        data_point = {'label': chunk_entry['label'], 'image': image, 'additional_info': chunk_entry['additional_info']}\n",
        "        dataset_part.append(data_point)\n",
        "\n",
        "    with open(os.path.join(output_dir, f'{mode}_{img_size}_part_{chunk_no + 1}.pickle'), 'wb') as f:\n",
        "        pickle.dump(dataset_part, f)\n",
        "\n",
        "mode = 'training'\n",
        "data_points = data_points_train\n",
        "\n",
        "data_points_chunks_generator = chunks(data_points, pickle_chunk_size)\n",
        "\n",
        "output_dir = os.path.join(f'./processed_data/HH25/{mode}')\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for chunk_no, chunk in enumerate(data_points_chunks_generator):\n",
        "    dataset_part = list()\n",
        "    for chunk_entry_no, chunk_entry in enumerate(chunk):\n",
        "        if (chunk_entry_no + 1) % 10 == 0:\n",
        "            print(f'Processing chunk entry no: {chunk_entry_no + 1}')\n",
        "\n",
        "        image = load_and_process_image(chunk_entry['path'], img_size)\n",
        "        data_point = {'label': chunk_entry['label'], 'image': image, 'additional_info': chunk_entry['additional_info']}\n",
        "        dataset_part.append(data_point)\n",
        "\n",
        "    with open(os.path.join(output_dir, f'{mode}_{img_size}_part_{chunk_no + 1}.pickle'), 'wb') as f:\n",
        "        pickle.dump(dataset_part, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjYZvTkDRp3G",
        "outputId": "fa53e1a4-83da-4336-a52e-3d96004d555a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk entry no: 10\n",
            "Processing chunk entry no: 20\n",
            "Processing chunk entry no: 30\n",
            "Processing chunk entry no: 40\n",
            "Processing chunk entry no: 50\n",
            "Processing chunk entry no: 60\n",
            "Processing chunk entry no: 70\n",
            "Processing chunk entry no: 80\n",
            "Processing chunk entry no: 90\n",
            "Processing chunk entry no: 100\n",
            "Processing chunk entry no: 110\n",
            "Processing chunk entry no: 120\n",
            "Processing chunk entry no: 130\n",
            "Processing chunk entry no: 140\n",
            "Processing chunk entry no: 150\n",
            "Processing chunk entry no: 160\n",
            "Processing chunk entry no: 10\n",
            "Processing chunk entry no: 20\n",
            "Processing chunk entry no: 30\n",
            "Processing chunk entry no: 40\n",
            "Processing chunk entry no: 50\n",
            "Processing chunk entry no: 60\n",
            "Processing chunk entry no: 70\n",
            "Processing chunk entry no: 80\n",
            "Processing chunk entry no: 90\n",
            "Processing chunk entry no: 100\n",
            "Processing chunk entry no: 110\n",
            "Processing chunk entry no: 120\n",
            "Processing chunk entry no: 130\n",
            "Processing chunk entry no: 140\n",
            "Processing chunk entry no: 150\n",
            "Processing chunk entry no: 160\n",
            "Processing chunk entry no: 10\n",
            "Processing chunk entry no: 20\n",
            "Processing chunk entry no: 30\n",
            "Processing chunk entry no: 40\n",
            "Processing chunk entry no: 50\n",
            "Processing chunk entry no: 60\n",
            "Processing chunk entry no: 70\n",
            "Processing chunk entry no: 80\n",
            "Processing chunk entry no: 90\n",
            "Processing chunk entry no: 100\n",
            "Processing chunk entry no: 110\n",
            "Processing chunk entry no: 120\n",
            "Processing chunk entry no: 130\n",
            "Processing chunk entry no: 140\n",
            "Processing chunk entry no: 150\n",
            "Processing chunk entry no: 160\n",
            "Processing chunk entry no: 170\n",
            "Processing chunk entry no: 180\n",
            "Processing chunk entry no: 190\n",
            "Processing chunk entry no: 200\n",
            "Processing chunk entry no: 210\n",
            "Processing chunk entry no: 220\n",
            "Processing chunk entry no: 230\n",
            "Processing chunk entry no: 240\n",
            "Processing chunk entry no: 250\n",
            "Processing chunk entry no: 260\n",
            "Processing chunk entry no: 270\n",
            "Processing chunk entry no: 280\n",
            "Processing chunk entry no: 290\n",
            "Processing chunk entry no: 300\n",
            "Processing chunk entry no: 310\n",
            "Processing chunk entry no: 320\n",
            "Processing chunk entry no: 330\n",
            "Processing chunk entry no: 340\n",
            "Processing chunk entry no: 350\n",
            "Processing chunk entry no: 360\n",
            "Processing chunk entry no: 370\n",
            "Processing chunk entry no: 380\n",
            "Processing chunk entry no: 390\n",
            "Processing chunk entry no: 400\n",
            "Processing chunk entry no: 410\n",
            "Processing chunk entry no: 420\n",
            "Processing chunk entry no: 430\n",
            "Processing chunk entry no: 440\n",
            "Processing chunk entry no: 450\n",
            "Processing chunk entry no: 460\n",
            "Processing chunk entry no: 470\n",
            "Processing chunk entry no: 480\n",
            "Processing chunk entry no: 490\n",
            "Processing chunk entry no: 500\n",
            "Processing chunk entry no: 510\n",
            "Processing chunk entry no: 520\n",
            "Processing chunk entry no: 530\n",
            "Processing chunk entry no: 540\n",
            "Processing chunk entry no: 550\n",
            "Processing chunk entry no: 560\n",
            "Processing chunk entry no: 570\n",
            "Processing chunk entry no: 580\n",
            "Processing chunk entry no: 590\n",
            "Processing chunk entry no: 600\n",
            "Processing chunk entry no: 610\n",
            "Processing chunk entry no: 620\n",
            "Processing chunk entry no: 630\n",
            "Processing chunk entry no: 640\n",
            "Processing chunk entry no: 650\n",
            "Processing chunk entry no: 660\n",
            "Processing chunk entry no: 670\n",
            "Processing chunk entry no: 680\n",
            "Processing chunk entry no: 690\n",
            "Processing chunk entry no: 700\n",
            "Processing chunk entry no: 710\n",
            "Processing chunk entry no: 720\n",
            "Processing chunk entry no: 730\n",
            "Processing chunk entry no: 740\n",
            "Processing chunk entry no: 750\n",
            "Processing chunk entry no: 760\n",
            "Processing chunk entry no: 770\n",
            "Processing chunk entry no: 780\n",
            "Processing chunk entry no: 790\n",
            "Processing chunk entry no: 800\n",
            "Processing chunk entry no: 810\n",
            "Processing chunk entry no: 820\n",
            "Processing chunk entry no: 830\n",
            "Processing chunk entry no: 840\n",
            "Processing chunk entry no: 850\n",
            "Processing chunk entry no: 860\n",
            "Processing chunk entry no: 870\n",
            "Processing chunk entry no: 880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "def load_pickle(filepath):\n",
        "  with open(filepath, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "  return data\n",
        "\n",
        "\n",
        "file_to_load = './training_512_part_1.pickle'\n",
        "train_hh19 = load_pickle(file_to_load)\n",
        "\n",
        "file_to_load = './testing_512_part_1.pickle'\n",
        "test_hh25 = load_pickle(file_to_load)\n",
        "\n",
        "file_to_load = './validation_512_part_1.pickle'\n",
        "validation_hh25 = load_pickle(file_to_load)\n",
        "\n",
        "\n",
        "print(train_hh19[0])\n",
        "print(test_hh25[0])\n",
        "print(validation_hh25[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEkgBWVFUQZx",
        "outputId": "1f7df811-c62b-4f82-c701-9b1a58faf915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 0, 'image': <PIL.Image.Image image mode=L size=512x512 at 0x7C4900004340>, 'additional_info': {'id': 16, 'grade': 5, 'real': 1, 'stage': 19}}\n",
            "{'label': 0, 'image': <PIL.Image.Image image mode=L size=512x512 at 0x7C4900007BE0>, 'additional_info': {'id': 52, 'grade': 5, 'real': 1, 'stage': 25}}\n",
            "{'label': 0, 'image': <PIL.Image.Image image mode=L size=512x512 at 0x7C4900238700>, 'additional_info': {'id': 66, 'grade': 4, 'real': 1, 'stage': 25}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_hh19_ids = [data_point['additional_info']['id'] for data_point in train_hh19]\n",
        "test_hh25_ids = [data_point['additional_info']['id'] for data_point in test_hh25]\n",
        "validation_hh25_ids = [data_point['additional_info']['id'] for data_point in validation_hh25]\n",
        "\n",
        "common_items = set(train_hh19_ids) & set(test_hh25_ids)\n",
        "\n",
        "if common_items:\n",
        "    print(\"Common items found:\", common_items)\n",
        "else:\n",
        "    print(\"No common items found\")"
      ],
      "metadata": {
        "id": "dUu9DGbzV5Mt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}